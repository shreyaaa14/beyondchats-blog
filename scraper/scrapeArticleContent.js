const axios = require("axios");
const cheerio = require("cheerio");
const Article = require("../models/Article");
const connectDB = require("../config/db");
require("dotenv").config();

const scrapeArticleContent = async () => {
  try {
    await connectDB();

    // 1ï¸âƒ£ Get articles with placeholder content
    const articles = await Article.find({
      content: "To be scraped later",
    });

    console.log(`ðŸ” Found ${articles.length} articles to update`);

    for (const article of articles) {
      try {
        // 2ï¸âƒ£ Fetch article page
        const { data } = await axios.get(article.sourceUrl);
        const $ = cheerio.load(data);

        // 3ï¸âƒ£ Extract text (paragraphs)
        let content = "";
        $("p").each((i, el) => {
          content += $(el).text().trim() + "\n\n";
        });

        if (content.length < 100) {
          console.log(`âš ï¸ Skipped (too short): ${article.title}`);
          continue;
        }

        // 4ï¸âƒ£ Update DB
        article.content = content.trim();
        article.isUpdated = true;
        await article.save();

        console.log(`âœ… Updated: ${article.title}`);
      } catch (err) {
        console.log(`âŒ Failed: ${article.title}`);
      }
    }

    console.log("ðŸŽ‰ Content scraping completed");
    process.exit();
  } catch (err) {
    console.error("âŒ Error:", err.message);
    process.exit(1);
  }
};

scrapeArticleContent();
